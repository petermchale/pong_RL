{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is a branch of machine learning in which the goal is for an agent (robot) to take a sequence of actions in an environment in such a way as to maximize a cumulative reward over repeated actions:\n",
    "\n",
    "![](images/action_observation_reward.png)\n",
    "\n",
    "The relation between actions and the environment is defined by a Policy. A policy is a rule that prescribes how the agent should act given that the environment is observed to be in a certain state. The policy may be stochastic, in which case it gives the probability of taking a certain action given a certain state. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari pong\n",
    "\n",
    "Let's consider an example of RL. In the game of Pong, you (the agent) compete against a game simulator (the environment) by moving a paddle up and down (the actions). One can set up an environment to simulate Pong by issuing the commands\n",
    "\n",
    "```\n",
    "brew install cmake boost boost-python sdl2 swig wget\n",
    "conda create -n openai python=2 ipython-notebook --yes \n",
    "source activate openai\n",
    "pip install 'gym[all]' \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym # Uses OpenAI Gym\n",
    "env = gym.make(\"Pong-v0\")\n",
    "\n",
    "number_time_points = 400\n",
    "\n",
    "from src import draw as dr\n",
    "from src import simulate as sim\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "dr.display_frames(sim.simulate_with_random_policy(env, number_time_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this video, the agent is in green. Once the agent decides which way to move the paddle, the game simulator executes the chosen action and updates the image on the screen. It also gives the agent a reward: +1 if the ball went past the opponent, -1 if the ball goes past the agent, and 0 otherwise. \n",
    "\n",
    "More formally, the game consists of a sequence of states $S^t$ (these will be closely related to the images), actions $A^t$, and rewards $R^t = R(S^t, A^t)$, all of which we model as random variables (hence the capital letters):\n",
    "\n",
    "\\begin{equation}\n",
    "S^0 \\xrightarrow[A^0, R^0]{} S^1 \\xrightarrow[A^1, R^1]{} \\mbox{ } \\cdots \n",
    "\\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Image processing\n",
    "\n",
    "We strip down the images returned by the game simulator to the bare minimum needed to make informed decisions about which actions to take:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = env.render(mode='rgb_array')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('original')\n",
    "ax2.imshow(dr.process(image), cmap='Greys')\n",
    "ax2.set_title('processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit assignment problem \n",
    "\n",
    "Associated with each realization of a state-action pair $(s, a)$ is a reward $R(s,a) \\in \\{-1, 0, +1\\}$. The goal is to find the policy $\\pi(a\\,|\\,s)$, representing the probability of choosing action $a$ in the state $s$, that maximizes the expected value of a cumulative reward over all time steps. \n",
    "\n",
    "What the cumulative reward should actually be is open to debate. For example, the pong game simulator assigns a reward of 0 when the agent's paddle hits the ball, even if the result is to launch the ball on a trajectory that evades the opponents paddle. Arguably, then, a positive (not zero) reward should be assigned to the agent's volley. This ambiguity in how to assign credit is widespread in RL problems and is known as the *credit assignment problem*. \n",
    "\n",
    "In the Pong game, a trajectory (or episode) is defined to be the time course up to the point at which the first player wins 21 games. We define the cumulative reward associated with a trajectory to be \n",
    "\n",
    "\\begin{equation} \n",
    "R_{tot} = \\sum_{games: i} \\left(\\gamma^{T_i}  + \\gamma^{T_i-1} + \\cdots + \\gamma + 1 \\right) R^{T_1 + \\sum_{j=2}^i (T_j+1)}\n",
    "\\end{equation}\n",
    "\n",
    "where $T_i$ is the duration of game $i$, $\\gamma<1$ is a discount factor, and $R^t = R(S^t, A^t)$ . In addition to assigning +1 when the ball goes past the opponent, for example, this formula assigns positive rewards to earlier timesteps in an exponentially decreasing manner. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning the optimal policy\n",
    "\n",
    "There are two ways to learn the policy with greatest rewards. The first, Dynamic Programming, makes the strong assumption that the agent knows the environment before experiencing it. That is, given the current state and action, the future state can be computed (or learned). \n",
    "\n",
    "In the second approach, Policy Optimization (also called Direct Policy Search), the agent gains knowledge of the environment only through its experience of it. This amounts to taking completely random actions and measuring the rewards that they induce. One then gradually increases the frequency with which actions are taken that by chance turned out to be advantageous in certain environmental states. This process of blindly searching policy space and choosing directions that lead to better rewards is similar to natural selection, which randomly explores genotype space while selecting fitter individuals at the expense of those less fit. And just like natural selection leads to unexpected adaptations in biology, Policy Optimization often works surprisingly well (e.g. Cross Entropy Method). \n",
    "\n",
    "But can we do better by leveraging additional information about the reward, viewed as a function of states (and possibly actions)?  Policy Gradient Methods find optimal policies by making use of the gradient of the reward function with respect to the parameters of the policy function. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient\n",
    "\n",
    "Consider the gradient $\\nabla_W E_\\tau [R_{tot}]$, where $W$ is a tensor of weights that parametrize the policy function $\\pi(a \\, | \\, s)$, and $\\tau$ is a random variable denoting a trajectory $\\{S^0, A^0; S^1, A^1; \\cdots ; S^T\\}$. Since $\\nabla_W E_\\tau [R_{tot}]$ indicates the direction in $W$-space in which $E_\\tau[R_{tot}]$ increases most rapidly, the gradient-ascent algorithm,\n",
    "\n",
    "\\begin{equation} \n",
    "W \\leftarrow W + \\eta \\nabla_W E_\\tau[R_{tot}],\n",
    "\\end{equation}\n",
    "\n",
    "converges to a set of parameters that define a locally optimal policy. A [clever algebraic manipulation](https://www.youtube.com/watch?v=yCqPMD6coO8&list=PLA89DCFA6ADACE599&index=20#t=23m) due to Williams allows us to write the gradient of the objective function in terms of the gradient of the policy\n",
    "\n",
    "\\begin{equation} \n",
    "\\nabla_W E_\\tau[R_{tot}] = E_\\tau\\left[\\, R_{tot} \\sum_{t=0}^T \\nabla_W \\log \\pi(A^t \\, | \\, S^t)\\,\\right].\n",
    "\\end{equation}\n",
    "\n",
    "In practice we estimate the expectation on the right-hand side by generating a small sample of trajectories and computing the sample mean. The variance in this estimator is proportional to the number of random variables involved ($T^2$), which can be reduced by a factor of two by a [clever algebraic trick](https://www.youtube.com/watch?v=oPGVsoBonLM#t=23m18s) to obtain: \n",
    "\n",
    "\\begin{equation} \n",
    "\\nabla_W E_\\tau[R_{tot}] = E_\\tau\\left[\\, \\sum_{t=0}^T \\nabla_W \\log \\pi(A^t \\, | \\, S^t) \n",
    "\\sum_{u=t}^T \n",
    "R_{\\text{disc}}^{u} \\,\\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $R^{t}_{\\text{disc}}$ is the discounted reward we have chosen to assign to time step $t$. \n",
    "\n",
    "The expression on the right-hand side makes intuitive sense: it tells us that we should shift $W$ in the direction that increases the probability of actions $\\left(\\nabla_W \\log \\pi\\right)$ that result in postive cumulative future discounted rewards $\\left(\\sum_{u=t}^T \n",
    "R_{\\text{disc}}^{u}\\right)$. Andrej Karpathy discusses this in more detail in his [blog](http://karpathy.github.io/2016/05/31/rl/) in the section entitled \"Deriving Policy Gradients\". \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the policy function\n",
    "\n",
    "To compute a formula for the policy gradient, $\\nabla_W \\log\\pi$, we need a model for the policy function $\\pi(a \\, | \\, s)$. \n",
    "Since there are only two possible actions, the policy function can be specified by the probability of moving the paddle up, $p(s)$:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "\\pi(\\uparrow | \\, s) & = & p(s) \\\\\n",
    "\\pi(\\downarrow | \\, s) & = & 1 - p(s).\n",
    "\\end{eqnarray}\n",
    "\n",
    "Following [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/), we use a three-layer feedforward neural network to model the function $p(s)$. \n",
    "\n",
    "\n",
    "![](images/NN_with_weights-01.png)\n",
    "\n",
    "\n",
    "A static image lacks important information for deciding how to move the paddle: the ball velocity. To extract some of that information, we will pass the *difference of successive  images* to the policy function. In other words, we choose the input nodes $s_j$ to be the difference of a pair of successive vectorized processed images. \n",
    "\n",
    "The layers of the neural network are connected via two weight matrices, $W^1$ and $W^2$ (no biases), whose initial values are set using Xavier normalization. Michael Nielsen gives a [nice discussion](http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization) of how Xavier normalization prevents neuron saturation and slowdown of learning. The math justifying the procedure can be found [here](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation \n",
    "\n",
    "The process of computing $p(s)$ is referred to as forward propagating through the network. The activations of the neurons in the hidden layer, $h_i$, is given by\n",
    "\n",
    "\\begin{equation} \n",
    "h = g(W^1s)\n",
    "\\end{equation} \n",
    "\n",
    "where $g(z)$ is the sigmoid function, and $s$ and $h$ are column vectors. The activation of the output neuron is\n",
    "\n",
    "\\begin{equation} \n",
    "p = g(W^2h).\n",
    "\\end{equation} \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation \n",
    "\n",
    "Having set up a model for the policy we may now compute its gradient, $\\nabla_W \\pi( a\\, | \\, s)$. Considering the two possible values of $a$ separately, and using the chain rule of differentiation, one may show that the matrix representing the gradient of the policy with respect to $W^2$ is\n",
    "\n",
    "\\begin{equation} \n",
    "\\nabla_{W^2} \\pi( a\\, | \\, s) = \\delta^3(s,a) h^{\\text{Tr}}(s) \n",
    "\\end{equation} \n",
    "\n",
    "where Tr denotes transpose and \n",
    "\n",
    "\\begin{eqnarray} \n",
    "\\delta^3(s,a) & = & y(a) - p(s) \\\\\n",
    "y(a) & = & \n",
    "\\left\\{ \n",
    "\\begin{array}{cc} \n",
    "1 & \\text{if } a = \\uparrow\\\\\n",
    "0 & \\text{if } a = \\downarrow\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{eqnarray} \n",
    "\n",
    "One way to interpret $\\delta^3$ is a measure of how sensitive the cross entropy $H = -y\\log p - (1-y)\\log(1-p)$ is to changes in the weighted input, $z^3 = W^2h$, to the output neuron: \n",
    "\n",
    "\\begin{equation} \n",
    "\\delta^3 = -\\nabla_{z^3} H.\n",
    "\\end{equation} \n",
    "\n",
    "Our task is to compute the reward-weighted sum of policy gradients $G_{W} = \\sum_{t=0}^T \\nabla_{W} \\log \\pi(a^t \\, | \\, s^t) \\, R_{\\text{part}}^{t}$ for a sample trajectory $\\{ s^0, a^0; s^1, a^1; \\cdots ; s^T \\}$, where the partial sum of future discounted rewards is $R_{\\text{part}}^{t} = \\sum_{u=t}^T R_{\\text{disc}}^{u}$. Using our expression for $\\nabla_{W^2} \\pi( a\\, | \\, s)$, we can now write\n",
    "\n",
    "\\begin{equation} \n",
    "\\boxed{\n",
    "G_{W^2} = \\left(\\hat{\\delta^3}\\right)^{\\text{Tr}} H\n",
    "}\n",
    "\\end{equation} \n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation} \n",
    "H = \n",
    "\\left[\n",
    "\\begin{array}{rcl} \n",
    "\\cdots & h^0 & \\cdots\\\\\n",
    "\\cdots & h^1 & \\cdots\\\\\n",
    " & \\vdots & \\\\\n",
    "\\cdots & h^T & \\cdots\\\\ \n",
    "\\end{array} \n",
    "\\right],\n",
    "\\end{equation}\n",
    "\n",
    "and the notation $\\cdots h^t \\cdots$ indicates that the vector of hidden-layer activations at time $t$, $h^t$, should be written as a row vector. Also, \n",
    "\n",
    "\\begin{equation} \n",
    "\\hat{\\delta^3} = \n",
    "\\left[\n",
    "\\begin{array}{c} \n",
    "\\delta^{3,0} \\, R^0_{\\text{part}} \\\\\n",
    "\\delta^{3,1} \\, R^1_{\\text{part}} \\\\\n",
    " \\vdots\\\\\n",
    "\\delta^{3,T} \\, R^T_{\\text{part}} \n",
    "\\end{array} \n",
    "\\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\delta^{3,t} = \\delta^3(s^t, a^t)$. \n",
    "\n",
    "A similar but more involved calculation yields\n",
    "\n",
    "\\begin{equation} \n",
    "\\boxed{\n",
    "G_{W^1} = \\left(\\hat{\\delta^2}\\right)^{\\text{Tr}}  S\n",
    "}\n",
    "\\end{equation} \n",
    "where \n",
    "\n",
    "\\begin{equation} \n",
    "S = \n",
    "\\left[\n",
    "\\begin{array}{rcl} \n",
    "\\cdots & s^0 & \\cdots\\\\\n",
    "\\cdots & s^1 & \\cdots\\\\\n",
    " & \\vdots & \\\\\n",
    "\\cdots & s^T & \\cdots\\\\ \n",
    "\\end{array} \n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "is the matrix formed by vertically stacking the states at each time point, with each state  written as a row vector. \n",
    "The error term $\\hat{\\delta^3}$ is 'back-propagated' to a similar error term, \n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "\\hat{\\delta^2} = \\left( \\hat{\\delta^3} \\otimes W^2 \\right) \\odot H \\odot \\left( 1 - H \\right),\n",
    "\\end{equation} \n",
    "\n",
    "associated with variations in inputs to the hidden layer. In this formula, the Outer Product is defined by\n",
    "\n",
    "\\begin{equation} \n",
    "(u \\otimes v)_{ij} = u_i v_j,\n",
    "\\end{equation}\n",
    "\n",
    "and the Hadamard Product by\n",
    "\n",
    "\\begin{equation} \n",
    "(A \\odot B)_{ij} = A_{ij} B_{ij} .\n",
    "\\end{equation} \n",
    "\n",
    "Finally, the weights of the neural network are updated using a mini-batch of trajectories: \n",
    "\n",
    "\\begin{equation} \n",
    "\\boxed{\n",
    "W \\leftarrow W + \\alpha \\sum_{\\tau_1, \\tau_2, \\cdots, \\tau_n} G^{\\tau_n}_W.\n",
    "}\n",
    "\\end{equation} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-07 14:54:18,935] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -0.0025585910396 -0.0403430932647\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 1.67304604 -1.68030948  1.67293877 ..., -0.01436487  0.00989354\n",
      "  0.0050286 ]\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[-1.58491447 -1.5845067   1.56743234 ...,  0.01434601  0.00981899\n",
      " -0.00499997]\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[-1.95200825 -1.95150266 -1.92843766 ...,  0.0142591  -0.00988352\n",
      "  0.00503999]\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 1.93617698  1.94465903 -1.94054941 ...,  0.01407295  0.00980177\n",
      " -0.00505598]\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 1.94464258  1.97331234 -1.96814356 ..., -0.01431337 -0.00983261\n",
      " -0.00507668]\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[-1.87153427  1.85508758  1.85433144 ...,  0.01434019 -0.00996083\n",
      "  0.00507652]\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[-1.94783848  1.93914161 -1.95556549 ...,  0.01418436 -0.00983246\n",
      " -0.00503331]\n",
      "0.5 0.0\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "[ 1.93928701  1.93951813 -1.93664636 ..., -0.01441839 -0.00994487\n",
      "  0.00512617]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c46075afa56b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mweights_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights.cPickle'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_game_outcomes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/petermchale/Dropbox/Data Science:Data Engineering preparation/ai project/RL/open ai/pong_RL/repo/src/policy.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(gamma, alpha, weights_filename, initialize_weights_using_checkpoint, render, print_game_outcomes)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_proc_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory_finished\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mstep_with_neural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_proc_previous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0ms_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/Dropbox/Data Science:Data Engineering preparation/ai project/RL/open ai/pong_RL/repo/src/policy.py\u001b[0m in \u001b[0;36mstep_with_neural_network\u001b[0;34m(image, image_proc_previous, env, weights, render, train)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mdelta3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrajectory_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/anaconda/envs/openai/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/anaconda/envs/openai/lib/python2.7/site-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/anaconda/envs/openai/lib/python2.7/site-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/anaconda/envs/openai/lib/python2.7/site-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/anaconda/envs/openai/lib/python2.7/site-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights_filename = 'weights.cPickle'\n",
    "number_time_points = 200\n",
    "\n",
    "from src import draw as dr \n",
    "from src import simulate as sim \n",
    "\n",
    "dr.display_frames(sim.simulate_with_trained_policy(weights_filename, number_time_points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TO DO: \n",
    "* plot training error as a function of batch number\n",
    "* revisit NG Notes to see if there are other metrics I should plot\n",
    "\n",
    "* add time correlation to reduce the jitteriness of paddle motion\n",
    "* run these experiments on Amazon GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle \n",
    "\n",
    "with open(weights_filename, 'rb') as file_in:\n",
    "    weights = cPickle.load(file_in)\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "x = np.array([[1,2],[3,4]])\n",
    "x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
