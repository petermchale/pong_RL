{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning (RL) is a branch of machine learning in which an agent (robot) learns how to act in an environment so as to receive positive rewards: \n",
    "\n",
    "![](images/action_observation_reward.png)\n",
    "\n",
    "The relation between actions and the environment is defined by a \"policy\": a rule that prescribes how the agent should act given that the environment is observed to be in a certain state. The policy may be stochastic, in which case it gives the probability of taking a certain action given a certain state. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari pong\n",
    "\n",
    "Let's consider an example of RL. In the game of Pong, you (the agent) compete against a game simulator (the environment) by moving a paddle up and down (the actions). One can set up an environment to simulate Pong by issuing the commands\n",
    "\n",
    "```\n",
    "brew install cmake boost boost-python sdl2 swig wget\n",
    "conda create -n openai python=2 ipython-notebook --yes \n",
    "source activate openai\n",
    "pip install 'gym[all]' \n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src import test\n",
    "from src import draw as dr\n",
    "\n",
    "%matplotlib inline\n",
    "dr.display_frames(test.simulate_with_random_policy(number_time_points=500, random_seed=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this video, the agent is in green. Once the agent decides which way to move the paddle, the game simulator executes the chosen action, computes the physics, and updates the image on the screen. It also gives the agent a reward: +1 if the ball went past the opponent, -1 if the ball goes past the agent, and 0 otherwise. \n",
    "\n",
    "More formally, let $T+1$ be the time at which the first player wins 21 games. Then a trajectory (or episode) is defined by the finite sequence \n",
    "\n",
    "\\begin{equation}\n",
    "S^0 \\xrightarrow[A^0, R^0]{} S^1 \\xrightarrow[A^1, R^1]{} \\mbox{ } \\cdots S^{T+1}.\n",
    "\\end{equation}\n",
    "\n",
    "Here, $S^t$ are states (these will be closely related to the images), $A^t$ are actions, and $R^t = R(S^t, A^t)$ are rewards. We adopt the convention that random variables are denoted by capital letters while realized values are denoted by lower-case letters. \n",
    "The actions are chosen stochastically, conditional upon the current state,\n",
    "\n",
    "\\begin{equation} \n",
    "P[ A^t = a^t \\,|\\, S^t = s^t] = \\pi(a^t \\,|\\, s^t) ,\n",
    "\\end{equation}\n",
    "\n",
    "while the next state $S^{t+1}$ is computed by the game simulator conditional upon the current state and current action. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit assignment problem \n",
    "\n",
    "When the agent's paddle hits the ball, the pong game simulator assigns a reward of zero. Yet, the result of that volley might be to launch the ball on a trajectory that evades the opponents paddle, winning the game. This suggests that some credit should be assigned to the agent's volley. Ambiguity in how to assign credit to actions is widespread in RL problems and is known as the *credit assignment problem*. \n",
    "\n",
    "We assign a discounted reward at time $t$ according to:\n",
    "\n",
    "\\begin{equation}\n",
    "R_{disc}^t = \\gamma^{G(t)-t} R^{G(t)}\n",
    "\\end{equation}\n",
    "\n",
    "where $G(t)$ is the game endpoint nearest to $t$ and $\\gamma<1$ is a discount factor. In addition to assigning +1 when the ball goes past the opponent, for example, this formula assigns positive rewards to earlier timesteps in an exponentially decreasing manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Log Likelihood\n",
    "\n",
    "The agent gains knowledge of the environment only through experience, i.e. taking  actions according to the current policy and measuring the rewards that they induce.  Consider one such experience: \n",
    "\n",
    "\\begin{equation}\n",
    "s^0 \\xrightarrow[a^0, r^0]{} s^1 \\xrightarrow[a^1, r^1]{} \\mbox{ } \\cdots s^{T+1}.\n",
    "\\end{equation}\n",
    "\n",
    "We would like to alter the policy function $\\pi$ so as to increase the probability of rare actions that resulted in positive discounted rewards while decreasing the probability of the frequent actions associated with negative discounted rewards. One way to do this is to alter $\\pi$ so that it increases the weighted log likelihood associated with this particular trajectory:\n",
    "\n",
    "\\begin{equation} \n",
    "\\boxed{\n",
    "{\\cal L} = \\sum_{t=0}^T \\log \\pi(a^t \\,|\\, s^t)  \\, R_{disc}^t .\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "To gain some intuition into this formula, it is helpful to think of the realized trajectory as a data set,\n",
    "\n",
    "\\begin{equation} \n",
    "\\tau = \\{(s^i, a^i): i=1\\cdots T\\},\n",
    "\\end{equation}\n",
    "\n",
    "and the policy function $\\pi$ as a model of the data. In this picture, sampling an action produces a predicted label $A^i$ associated with the $i$th data point. \n",
    "Then, if all the discounted rewards are equal, ${\\cal L}$ is proportional to the\n",
    "probability that the model produces predicted labels that coincide with the observed labels: \n",
    "\n",
    "\\begin{equation} \n",
    "{\\cal L} \\propto \\log P[ A^0 = a^0, \\cdots , A^T = a^T \\,|\\, S^0 = s^0 , \\cdots , S^T = s^T] , \n",
    "\\end{equation} \n",
    "\n",
    "i.e. the log likelihood familiar to us from supervised learning (though in RL a model is used not only to fit data but also to generate them).\n",
    "The constant of proportionality in this formula is the common reward for each time step. If that reward is positive, then the observed sequence of actions is favorable (for this particular sequence of states), and increasing ${\\cal L}$ amounts to increasing the chances of choosing those actions in those states in the future. Conversely, if the common reward is negative, then the observed sequence of actions is unfavorable (again for the given sequence of states), and increasing ${\\cal L}$ amounts to decreasing the chances of taking those actions in the future. \n",
    "\n",
    "Another advantage of using weighted log likelihood as a measure of performance is that increasing it tends to acclerate wins (by minimizing the number of actions with positive discounted rewards) while at the same time delaying losses (by maximizing the number of actions with negative discounted rewards). \n",
    "\n",
    "You may find further insights into the meaning of $\\cal L$ in [Andrej Karpathy's blog](http://karpathy.github.io/2016/05/31/rl/) in the sections entitled \"Update: December 9, 2016 - alternative view\" and \"Deriving Policy Gradients\". \n",
    "\n",
    "How might we increase the weighted log likelihood? A brute-force approach is to randomly explore policy function space, biasing the search towards higher-performant regions, as done in the [Cross Entropy Method](https://www.aaai.org/Papers/ICML/2003/ICML03-068.pdf). This is similar to natural selection, which randomly explores genotype space, selecting fitter individuals at the expense of those less fit. And just like natural selection leads to unexpected adaptations in biology, the Cross Entropy Methods often works surprisingly well.\n",
    "\n",
    "Another way to increase $\\cal L$ is the Policy Gradient Method: greedily increase $\\cal L$ by adjusting the parameters $W$ of the policy function in the direction of $\\nabla_W {\\cal L}$,\n",
    "\n",
    "\\begin{equation} \n",
    "\\boxed{\n",
    "W \\leftarrow W + \\alpha \\frac{1}{n} \\sum_{i=1}^n\n",
    "\\nabla_W {\\cal L(\\tau_i)} ,\n",
    "}\n",
    "\\end{equation} \n",
    "\n",
    "where we have reduced the variance in the gradient  by averaging over a mini-batch of trajectories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling the policy function\n",
    "\n",
    "To compute a formula for the policy gradient, $\\nabla_W \\log\\pi$, we need a model for the policy function $\\pi(a \\, | \\, s)$. \n",
    "Since there are only two possible actions, the policy function can be specified by the probability of moving the paddle up, $p(s)$:\n",
    "\n",
    "\\begin{eqnarray} \n",
    "\\pi(\\uparrow | \\, s) & = & p(s) \\\\\n",
    "\\pi(\\downarrow | \\, s) & = & 1 - p(s).\n",
    "\\end{eqnarray}\n",
    "\n",
    "Following [Andrej Karpathy](http://karpathy.github.io/2016/05/31/rl/), we use a three-layer feedforward neural network to model the function $p(s)$. \n",
    "\n",
    "\n",
    "![](images/NN_with_weights-01.png)\n",
    "\n",
    "\n",
    "The layers of the neural network are connected via two weight matrices, $W^1$ and $W^2$ (no biases), whose initial values are set using Xavier normalization. Michael Nielsen gives a [nice discussion](http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization) of how Xavier normalization prevents neuron saturation and slowdown of learning. The math justifying the procedure can be found [here](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization).\n",
    "\n",
    "\n",
    "A static image lacks important information for deciding how to move the paddle: the ball velocity. To extract some of that information, we will pass the *difference of successive  images* to the policy function. In other words, we choose the input nodes $s_j$ to be the difference of a pair of successive vectorized processed images. \n",
    "\n",
    "We process the images by stripping them down to the bare minimum needed to make informed decisions about which actions to take:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src import draw as dr\n",
    "image = dr.sample_image(random_seed=0) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "ax1.imshow(image)\n",
    "ax1.set_title('original')\n",
    "ax2.imshow(dr.process(image), cmap='Greys')\n",
    "ax2.set_title('processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation \n",
    "\n",
    "The process of computing $p(s)$ is referred to as forward propagating through the network. We define the neurons in the hidden layer, $h_i$, to be Rectified Linear Units (ReLUs),\n",
    "\n",
    "\\begin{equation} \n",
    "h = \\max(0, W^1s),\n",
    "\\end{equation} \n",
    "\n",
    "where $s$ and $h$ are column vectors. ReLUs can accelerate training relative to saturating neurons by (partially) circumventing the vanishing gradient problem (e.g. Fig 1 of [Krizhevsky 2012](Krizhevsky 2012.pdf)). \n",
    "Since probabilities must lie between $0$ and $1$, we model \n",
    "the activation of the output neuron with a sigmoid function $g$ according to:\n",
    "\n",
    "\\begin{equation} \n",
    "p = g(W^2h).\n",
    "\\end{equation} \n",
    "\n",
    "By defining our objective function ${\\cal L}$ in terms of a cross-entropy ($\\log\\pi$), we avoid the problem of learning slowdown that would otherwise be associated with the fact that $g$ saturates (see Eq. (67) [here](http://neuralnetworksanddeeplearning.com/chap3.html)). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation \n",
    "\n",
    "Having set up a model for the policy we may now compute its gradient, $\\nabla_W \\log \\pi( a\\, | \\, s)$. Considering the two possible values of $a$ separately, and using the chain rule of differentiation, one may show that the matrix representing the gradient of the policy with respect to $W^2$ (the weights coming out of the second layer of the policy network) is\n",
    "\n",
    "\\begin{equation} \n",
    "\\nabla_{W^2} \\log \\pi( a\\, | \\, s) = \\delta^3(s,a) h^{\\text{Tr}}(s) \n",
    "\\end{equation} \n",
    "\n",
    "where Tr denotes transpose. The first factor on the right-hand side is defined by\n",
    "\n",
    "\\begin{eqnarray} \n",
    "\\delta^3(s,a) & = & y(a) - p(s) \\\\\n",
    "y(a) & = &\n",
    "\\left\\{ \n",
    "\\begin{array}{cc}\n",
    "1 & \\text{ if } a = \\uparrow \\\\\n",
    "0 & \\text{ if } a = \\downarrow\n",
    "\\end{array}\n",
    "\\right. .\n",
    "\\end{eqnarray} \n",
    "\n",
    "and measures the sensitivity of $\\log \\pi(a \\, | \\, s)$ to changes in the weighted input to the output neuron. \n",
    "\n",
    "We may now compute $\\nabla_{W} {\\cal L}(\\tau)$ for a sample trajectory $\\tau = \\{ s^0, a^0; s^1, a^1; \\cdots ; s^T \\}$ as:\n",
    "\n",
    "\\begin{equation} \n",
    "\\boxed{\n",
    "\\nabla_{W^2} {\\cal L} = \\left(\\hat{\\delta^3}\\right)^{\\text{Tr}} H\n",
    "}\n",
    "\\end{equation} \n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation} \n",
    "H = \n",
    "\\left[\n",
    "\\begin{array}{rcl} \n",
    "\\cdots & h^0 & \\cdots\\\\\n",
    "\\cdots & h^1 & \\cdots\\\\\n",
    " & \\vdots & \\\\\n",
    "\\cdots & h^T & \\cdots\\\\ \n",
    "\\end{array} \n",
    "\\right],\n",
    "\\end{equation}\n",
    "\n",
    "and the notation $\\cdots h^t \\cdots$ indicates that the vector of hidden-layer activations at time $t$, $h^t$, should be written as a row vector. Also, \n",
    "\n",
    "\\begin{equation} \n",
    "\\hat{\\delta^3} = \n",
    "\\left[\n",
    "\\begin{array}{c} \n",
    "\\delta^{3,0} \\, R^0_{\\text{disc}} \\\\\n",
    "\\delta^{3,1} \\, R^1_{\\text{disc}} \\\\\n",
    " \\vdots\\\\\n",
    "\\delta^{3,T} \\, R^T_{\\text{disc}} \n",
    "\\end{array} \n",
    "\\right],\n",
    "\\end{equation}\n",
    "\n",
    "where $\\delta^{3,t} = \\delta^3(s^t, a^t)$. \n",
    "\n",
    "A similar but more involved calculation yields\n",
    "\n",
    "\\begin{equation} \n",
    "\\boxed{\n",
    "\\nabla_{W^1} {\\cal L}  = \\left(\\hat{\\delta^2}\\right)^{\\text{Tr}}  S\n",
    "}\n",
    "\\end{equation} \n",
    "where \n",
    "\n",
    "\\begin{equation} \n",
    "S = \n",
    "\\left[\n",
    "\\begin{array}{rcl} \n",
    "\\cdots & s^0 & \\cdots\\\\\n",
    "\\cdots & s^1 & \\cdots\\\\\n",
    " & \\vdots & \\\\\n",
    "\\cdots & s^T & \\cdots\\\\ \n",
    "\\end{array} \n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "is the matrix formed by vertically stacking the states at each time point, with each state  written as a row vector. \n",
    "The error term $\\hat{\\delta^3}$ is 'back-propagated' to a similar error term, \n",
    "\n",
    "\n",
    "\\begin{equation} \n",
    "\\hat{\\delta^2} = \\left( \\hat{\\delta^3} \\otimes W^2 \\right) \\odot u(H),\n",
    "\\end{equation} \n",
    "\n",
    "associated with variations in inputs to the hidden layer. In this formula, \n",
    "the Outer Product is defined by\n",
    "\n",
    "\\begin{equation} \n",
    "(u \\otimes v)_{ij} = u_i v_j,\n",
    "\\end{equation}\n",
    "\n",
    "and the Hadamard Product by\n",
    "\n",
    "\\begin{equation} \n",
    "(A \\odot B)_{ij} = A_{ij} B_{ij} .\n",
    "\\end{equation} \n",
    "\n",
    "The function $u$ is the Heaviside function, \n",
    "\n",
    "\\begin{equation} \n",
    "u(h) = \n",
    "\\left\\{\n",
    "\\begin{array}{cc} \n",
    "0 & h = 0 \\\\\n",
    "1 & h > 0 \n",
    "\\end{array}\n",
    "\\right. ,\n",
    "\\end{equation} \n",
    "\n",
    "and comes from back-propagating through ReLUs in the hidden layer. It tells us that, should a hidden neuron ever become zero (dead) at all time points in a trajectory, then $\\nabla_{W^1} {\\cal L}=0$ for all the associated incoming weights $W^1$, which therefore do not change when a gradient descent update is performed. To the extent that the neuronal activations experienced during the trajectory were typical, the hidden neuron is therefore destined to remain dead, and no learning associated with this neuron is possible. This is a well-known problem associated with ReLU neurons. It can be monitored by reporting the number of dead neurons as training progresses, and mitigated by reducing the learning rate $\\alpha$ should the number of dead neurons become unacceptably high. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-17 22:30:46,584] Making new env: Pong-v0\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'weights.cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6804eb9aaf9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## TO DO:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_with_trained_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weights.cPickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_time_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/petermchale/Dropbox/Data Science:Data Engineering preparation/ai project/RL/open ai/pong_RL/repo/src/test.pyc\u001b[0m in \u001b[0;36msimulate_with_trained_policy\u001b[0;34m(weights_filename, number_time_points, random_seed)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_processed_previous\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/Dropbox/Data Science:Data Engineering preparation/ai project/RL/open ai/pong_RL/repo/src/policy.pyc\u001b[0m in \u001b[0;36minitialize_testing\u001b[0;34m(weights_filename, random_seed)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_processed_previous\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_rest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_weights_using_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_processed_previous\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/petermchale/Dropbox/Data Science:Data Engineering preparation/ai project/RL/open ai/pong_RL/repo/src/policy.pyc\u001b[0m in \u001b[0;36minitialize_weights_using_checkpoint\u001b[0;34m(weights_filename)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minitialize_weights_using_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'weights.cPickle'"
     ]
    }
   ],
   "source": [
    "from src import test\n",
    "from src import draw as dr\n",
    "\n",
    "## TO DO: \n",
    "\n",
    "dr.display_frames(test.simulate_with_trained_policy(weights_filename='weights.cPickle', number_time_points=5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TO DO: \n",
    "\n",
    "* TF: run this in the cloud: https://gist.github.com/greydanus/5036f784eec2036252e1990da21eda18 (TF computes the gradient once I give it the ReLU forward prop and the loss function); then edit my code to run TF\n",
    "* numpy + mxnet: http://minpy.readthedocs.io/en/latest/tutorial/rl_policy_gradient_tutorial/rl_policy_gradient.html\n",
    "* explore keras: https://github.com/umutisik/gan-experiments\n",
    "* openai + TF: https://github.com/openai/universe-starter-agent#atari-pong\n",
    "\n",
    "## Wrap up\n",
    "\n",
    "create section entitled \"Training/Learning\" and insert graph of rewards versus epoch (vary the parameter beta of the moving average between 0 and 1), L versus epoch (point out that rewards could be high but L could be low because of chance) and video of trained agent playing pong into notebook; provide some details about simulation parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
